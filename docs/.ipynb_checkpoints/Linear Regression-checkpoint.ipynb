{
 "metadata": {
  "language": "Julia",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Describing and implementing multivariate linear regression.<br><br>\n",
      "Definitions:<br>\n",
      "Let the set of training examples $T = \\{(\\vec{x_{i}}, y_i)\\}$ for $0 \\leq i \\leq m$<br>\n",
      "where $\\vec{x_{i}} \\in \\mathbb{R}^n$ is the $i^\\text{th}$ training example input, and $n$ is the number of features, $y_i \\in \\mathbb{R}$ is $i^\\text{th}$ is the training output.<br>\n",
      "Let $x_{i,j}$ denote the $j^\\text{th}$ feature of the $i^\\text{th}$ training example.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The linear regression hypothesis function $h_\\vec{\\theta} : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$h_\\vec{\\theta}(\\vec{x}) = \\sum_{i=0}^{n} \\theta_{i} x_{i} = \\vec{\\theta}^{T} \\vec{x}$$\n",
      "Where we take $x_0 = 1$, and let $\\vec{\\theta} \\in \\mathbb{R}^{n+1}$ be the parameter vector.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cost function $J : \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$J(\\vec{\\theta}) = \\frac{1}{2 m}\\sum_{i=1}^{m}(h_\\vec{\\theta}(\\vec{x}_{i}) - y_i)^2 = \\frac{1}{2 m}\\sum_{i=1}^{m}(\\vec{\\theta}^{T} \\vec{x} - y_i)^2$$<br>\n",
      "\n",
      "We want to minimize the cost function $J$ by varying the parameter vector $\\vec{\\theta}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, we can analytically solve this problem, but we could also use an optimization algorithm like gradient descent. I'll go over both ways."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Analytic Minimum Cost Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For cost function:\n",
      "$$J(\\vec{\\theta}) = \\frac{1}{2 m}\\sum_{i=1}^{m}(\\vec{\\theta}^{T} \\vec{x} - y_i)^2$$<br>\n",
      "We want to solve for $(\\theta_0, \\theta_1, \\dotsc, \\theta_{n})^T$ such that:<br>\n",
      "$\\frac{\\partial}{\\partial \\theta_i}{J(\\vec{\\theta})} = 0$ for all $i \\in \\{0,1,...,n\\}$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gradient Descent Intuition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One method to find the parameter vector is the gradient descent algorithm.<br><br>\n",
      "\n",
      "The idea in gradient descent is similar to the idea in single variable calculus:<br>\n",
      "<img src=\"files/images/deriv.png\"><br>\n",
      "Where we know that $\\frac{d}{dx} f(x) = 0$ at the extrema (points A and B), and $\\frac{d}{dx} f(x)$ can otherwise be used direct us toward the nearest (local) minimum/maximum (e.g. the slope of the lines at points C, D, E, F).<br><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a multivariate function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$, we use the vector differential, or gradient: $\\vec{\\nabla} f(x)$ to guide us to a minimum.<br>For example, in:\n",
      "<img src=\"files/images/gradient.png\" width=\"400px\"><br>\n",
      "A gradient vector field is shown projected beneath a plot of the function $f(x,y) = \u2212(\\cos^2 x + \\cos^2 y)^2.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the vectors are pointed to the direction of maximum ascent--i.e. away from the minimum, thus if we want to minimize our cost function $J$ in the linear regression problem, we need to look toward the opposite direction: $-\\vec{\\nabla} f(x)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how we actually follow that direction, consider the Taylor expansion of $f(\\vec{x}_0 + h\\vec{\\epsilon})$ where $\\epsilon \\in \\mathbb{R}^{n+1}$ is a unit-vector that we want to minimize in the direction of if we start at vector $\\vec{x}_0$, and $h \\in \\mathbb{R}$ is a constant \"step-size\":<br>\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) = f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} + h^2\\text{(const. error)}\n",
      "$$\n",
      "<br><br>\n",
      "If we take $h$ to be very small such that $h^2$ is even smaller, then let's regard $h^2\\text{(const. error)} \\approx 0$ so that:\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) \\approx f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to choose a vector $\\vec{\\epsilon}$ that minimizes $f(\\vec{x}_0 + h\\vec{\\epsilon})$.<br>\n",
      "i.e. that minimizes\n",
      "$f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}$.<br><br>\n",
      "\n",
      "Note that $\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} = \\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert \\lvert \\vec{\\epsilon} \\rvert \\cos \\delta$ where $\\delta$ is the angle between $\\vec{\\nabla} f(\\vec{x}_0)$ and $\\vec{\\epsilon}$.<br>\n",
      "By choosing unit-vector $\\vec{\\epsilon} = \\frac{-\\vec{\\nabla} f(\\vec{x}_0)}{\\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert}$, we have $\\cos(\\delta) = -1$, its least value. <br><br>\n",
      "\n",
      "So taking:\n",
      "$\\vec{x_1} = \\vec{x}_0 + h\\vec{\\epsilon} = \\vec{x}_0 - h\\vec{\\nabla} f(\\vec{x}_0)$ for some (possibly varying*) step-size $h$,<br>\n",
      "and inspecting whether $f(\\vec{x_{i+1}}) - f(\\vec{x_{i}})$ reaches convergence within some threshold, we have the basic gradient descent algorithm.<br><br>\n",
      "\n",
      "*What to set the $h$ values to is another complication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also see: http://mathworld.wolfram.com/MethodofSteepestDescent.html <br>\n",
      "\n",
      "Image sources:<br>\n",
      "http://www.themathpage.com/acalc/calc_IMG/060.png<br>\n",
      "http://en.wikipedia.org/wiki/File:Gradient99.png<br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}