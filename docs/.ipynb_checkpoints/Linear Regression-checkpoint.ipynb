{
 "metadata": {
  "language": "Julia",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Describing and implementing multivariate linear regression.<br><br>\n",
      "Definitions:<br>\n",
      "Let the set of training examples $T = \\{(\\vec{x_{i}}, y_i)\\}$ for $0 \\leq i \\leq m$<br>\n",
      "where $\\vec{x_{i}} \\in \\mathbb{R}^n$ is the $i^\\text{th}$ training example input, and $n$ is the number of features, $y_i \\in \\mathbb{R}$ is $i^\\text{th}$ is the training output.<br>\n",
      "Let $x_{i,j}$ denote the $j^\\text{th}$ feature of the $i^\\text{th}$ training example.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The linear regression hypothesis function $h_\\vec{\\theta} : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$h_\\vec{\\theta}(\\vec{x}) = \\sum_{i=0}^{n} \\theta_{i} x_{i} = \\vec{\\theta}^{T} \\vec{x}$$\n",
      "Where we take $x_0 = 1$, and let $\\vec{\\theta} \\in \\mathbb{R}^{n+1}$ be the parameter vector.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cost function $J : \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$J(\\vec{\\theta}) = \\frac{1}{2 m}\\sum_{i=1}^{m}(h_\\vec{\\theta}(\\vec{x}_{i}) - y_i)^2 = \\frac{1}{2 m}\\sum_{i=1}^{m}(\\vec{\\theta}^{T} \\vec{x} - y_i)^2$$<br>\n",
      "\n",
      "We want to minimize the cost function $J$ by varying the parameter vector $\\vec{\\theta}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, we can analytically solve this problem by solving the system of equations for $(\\theta_0, \\theta_1, \\dotsc, \\theta_{n})^T$ given by $\\frac{\\partial}{\\partial \\theta_i}{J(\\vec{\\theta})} = 0$ for all $i \\in \\{0,1,...,n\\}$.<br>\n",
      "\n",
      "\n",
      "We can alternatively use an optimization algorithm like gradient descent, and we can recycle it for other min/max problems; so I'll go over that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One method to find the parameter vector is the gradient descent algorithm.<br><br>\n",
      "\n",
      "The idea in gradient descent is similar to the idea in single variable calculus:<br>\n",
      "<img src=\"files/images/deriv.png\"><br>\n",
      "Where we know that $\\frac{d}{dx} f(x) = 0$ at the extrema (points A and B), and $\\frac{d}{dx} f(x)$ can otherwise be used direct us toward the nearest (local) minimum/maximum (e.g. the slope of the lines at points C, D, E, F).<br><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a multivariate function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$, we use the vector differential, or gradient: $\\vec{\\nabla} f(x)$ to guide us to a minimum.<br>For example, in:\n",
      "<img src=\"files/images/gradient.png\" width=\"400px\"><br>\n",
      "A gradient vector field is shown projected beneath a plot of the function $f(x,y) = \u2212(\\cos^2 x + \\cos^2 y)^2.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the vectors are pointed to the direction of maximum ascent--i.e. away from the minimum, thus if we want to minimize our cost function $J$ in the linear regression problem, we need to look toward the opposite direction: $-\\vec{\\nabla} f(x)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how we actually follow that direction, consider the Taylor expansion of $f(\\vec{x}_0 + h\\vec{\\epsilon})$ where $\\epsilon \\in \\mathbb{R}^{n+1}$ is a unit-vector that we want to minimize in the direction of if we start at vector $\\vec{x}_0$, and $h \\in \\mathbb{R}$ is a constant \"step-size\":<br>\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) = f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} + h^2\\text{(const. error)}\n",
      "$$\n",
      "<br><br>\n",
      "If we take $h$ to be very small such that $h^2$ is even smaller, then let's regard $h^2\\text{(const. error)} \\approx 0$ so that:\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) \\approx f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to choose a vector $\\vec{\\epsilon}$ that minimizes $f(\\vec{x}_0 + h\\vec{\\epsilon})$.<br>\n",
      "i.e. that minimizes\n",
      "$f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}$.<br><br>\n",
      "\n",
      "Note that $\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} = \\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert \\lvert \\vec{\\epsilon} \\rvert \\cos \\delta$ where $\\delta$ is the angle between $\\vec{\\nabla} f(\\vec{x}_0)$ and $\\vec{\\epsilon}$.<br>\n",
      "By choosing unit-vector $\\vec{\\epsilon} = \\frac{-\\vec{\\nabla} f(\\vec{x}_0)}{\\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert}$, we have $\\cos(\\delta) = -1$, its least value. <br><br>\n",
      "\n",
      "So taking:\n",
      "$\\vec{x_1} = \\vec{x}_0 + h\\vec{\\epsilon} = \\vec{x}_0 - h\\vec{\\nabla} f(\\vec{x}_0)$ for some (possibly varying*) step-size $h$,<br>\n",
      "and inspecting whether $f(\\vec{x_{i+1}}) - f(\\vec{x_{i}})$ reaches convergence within some threshold, we have the basic gradient descent algorithm.<br><br>\n",
      "\n",
      "*What to set the $h$ values to is another complication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also see: http://mathworld.wolfram.com/MethodofSteepestDescent.html <br>\n",
      "\n",
      "Image sources:<br>\n",
      "http://www.themathpage.com/acalc/calc_IMG/060.png<br>\n",
      "http://en.wikipedia.org/wiki/File:Gradient99.png<br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mv_linear_regression(training_set)\n",
      "    # Training_set assumed to be a dataframe\n",
      "    # For which the last column is the output variable.\n",
      "    \n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "# methods for generic function linear_regression\n",
        "linear_regression(training_set)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using RDatasets\n",
      "\n",
      "iris = data(\"datasets\", \"iris\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "150x6 DataFrame:\n",
        "              Sepal.Length Sepal.Width Petal.Length Petal.Width     Species\n",
        "[1,]        1          5.1         3.5          1.4         0.2    \"setosa\"\n",
        "[2,]        2          4.9         3.0          1.4         0.2    \"setosa\"\n",
        "[3,]        3          4.7         3.2          1.3         0.2    \"setosa\"\n",
        "[4,]        4          4.6         3.1          1.5         0.2    \"setosa\"\n",
        "[5,]        5          5.0         3.6          1.4         0.2    \"setosa\"\n",
        "[6,]        6          5.4         3.9          1.7         0.4    \"setosa\"\n",
        "[7,]        7          4.6         3.4          1.4         0.3    \"setosa\"\n",
        "[8,]        8          5.0         3.4          1.5         0.2    \"setosa\"\n",
        "[9,]        9          4.4         2.9          1.4         0.2    \"setosa\"\n",
        "[10,]      10          4.9         3.1          1.5         0.1    \"setosa\"\n",
        "[11,]      11          5.4         3.7          1.5         0.2    \"setosa\"\n",
        "[12,]      12          4.8         3.4          1.6         0.2    \"setosa\"\n",
        "[13,]      13          4.8         3.0          1.4         0.1    \"setosa\"\n",
        "[14,]      14          4.3         3.0          1.1         0.1    \"setosa\"\n",
        "[15,]      15          5.8         4.0          1.2         0.2    \"setosa\"\n",
        "[16,]      16          5.7         4.4          1.5         0.4    \"setosa\"\n",
        "[17,]      17          5.4         3.9          1.3         0.4    \"setosa\"\n",
        "[18,]      18          5.1         3.5          1.4         0.3    \"setosa\"\n",
        "[19,]      19          5.7         3.8          1.7         0.3    \"setosa\"\n",
        "[20,]      20          5.1         3.8          1.5         0.3    \"setosa\"\n",
        "  :\n",
        "[131,]    131          7.4         2.8          6.1         1.9 \"virginica\"\n",
        "[132,]    132          7.9         3.8          6.4         2.0 \"virginica\"\n",
        "[133,]    133          6.4         2.8          5.6         2.2 \"virginica\"\n",
        "[134,]    134          6.3         2.8          5.1         1.5 \"virginica\"\n",
        "[135,]    135          6.1         2.6          5.6         1.4 \"virginica\"\n",
        "[136,]    136          7.7         3.0          6.1         2.3 \"virginica\"\n",
        "[137,]    137          6.3         3.4          5.6         2.4 \"virginica\"\n",
        "[138,]    138          6.4         3.1          5.5         1.8 \"virginica\"\n",
        "[139,]    139          6.0         3.0          4.8         1.8 \"virginica\"\n",
        "[140,]    140          6.9         3.1          5.4         2.1 \"virginica\"\n",
        "[141,]    141          6.7         3.1          5.6         2.4 \"virginica\"\n",
        "[142,]    142          6.9         3.1          5.1         2.3 \"virginica\"\n",
        "[143,]    143          5.8         2.7          5.1         1.9 \"virginica\"\n",
        "[144,]    144          6.8         3.2          5.9         2.3 \"virginica\"\n",
        "[145,]    145          6.7         3.3          5.7         2.5 \"virginica\"\n",
        "[146,]    146          6.7         3.0          5.2         2.3 \"virginica\"\n",
        "[147,]    147          6.3         2.5          5.0         1.9 \"virginica\"\n",
        "[148,]    148          6.5         3.0          5.2         2.0 \"virginica\"\n",
        "[149,]    149          6.2         3.4          5.4         2.3 \"virginica\"\n",
        "[150,]    150          5.9         3.0          5.1         1.8 \"virginica\"\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's predict petal width as a function of\n",
      "# sepal.length, sepal.width, and petal.length.\n",
      "iris[:(Species .== \"setosa\"), :][[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]]\n",
      "#pw_df = DataFrame"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "50x4 DataFrame:\n",
        "         Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
        "[1,]              5.1         3.5          1.4         0.2\n",
        "[2,]              4.9         3.0          1.4         0.2\n",
        "[3,]              4.7         3.2          1.3         0.2\n",
        "[4,]              4.6         3.1          1.5         0.2\n",
        "[5,]              5.0         3.6          1.4         0.2\n",
        "[6,]              5.4         3.9          1.7         0.4\n",
        "[7,]              4.6         3.4          1.4         0.3\n",
        "[8,]              5.0         3.4          1.5         0.2\n",
        "[9,]              4.4         2.9          1.4         0.2\n",
        "[10,]             4.9         3.1          1.5         0.1\n",
        "[11,]             5.4         3.7          1.5         0.2\n",
        "[12,]             4.8         3.4          1.6         0.2\n",
        "[13,]             4.8         3.0          1.4         0.1\n",
        "[14,]             4.3         3.0          1.1         0.1\n",
        "[15,]             5.8         4.0          1.2         0.2\n",
        "[16,]             5.7         4.4          1.5         0.4\n",
        "[17,]             5.4         3.9          1.3         0.4\n",
        "[18,]             5.1         3.5          1.4         0.3\n",
        "[19,]             5.7         3.8          1.7         0.3\n",
        "[20,]             5.1         3.8          1.5         0.3\n",
        "  :\n",
        "[31,]             4.8         3.1          1.6         0.2\n",
        "[32,]             5.4         3.4          1.5         0.4\n",
        "[33,]             5.2         4.1          1.5         0.1\n",
        "[34,]             5.5         4.2          1.4         0.2\n",
        "[35,]             4.9         3.1          1.5         0.2\n",
        "[36,]             5.0         3.2          1.2         0.2\n",
        "[37,]             5.5         3.5          1.3         0.2\n",
        "[38,]             4.9         3.6          1.4         0.1\n",
        "[39,]             4.4         3.0          1.3         0.2\n",
        "[40,]             5.1         3.4          1.5         0.2\n",
        "[41,]             5.0         3.5          1.3         0.3\n",
        "[42,]             4.5         2.3          1.3         0.3\n",
        "[43,]             4.4         3.2          1.3         0.2\n",
        "[44,]             5.0         3.5          1.6         0.6\n",
        "[45,]             5.1         3.8          1.9         0.4\n",
        "[46,]             4.8         3.0          1.4         0.3\n",
        "[47,]             5.1         3.8          1.6         0.2\n",
        "[48,]             4.6         3.2          1.4         0.2\n",
        "[49,]             5.3         3.7          1.5         0.2\n",
        "[50,]             5.0         3.3          1.4         0.2\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}