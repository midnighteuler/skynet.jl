{
 "metadata": {
  "language": "Julia",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Describing and implementing multivariate linear regression.<br><br>\n",
      "Definitions:<br>\n",
      "Let the set of $m$ training examples be $T = \\{(\\vec{x_{i}}, y_i)\\}$ for $0 \\leq i \\leq m - 1$<br>\n",
      "where $\\vec{x_{i}} \\in \\mathbb{R}^n$ is the $i^\\text{th}$ training example input, and $n$ is the number of features, $y_i \\in \\mathbb{R}$ is $i^\\text{th}$ is the training output.<br>\n",
      "Let $x_{i,j}$ denote the $j^\\text{th}$ feature of the $i^\\text{th}$ training example.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The linear regression hypothesis function $h_\\vec{\\theta} : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$h_\\vec{\\theta}(\\vec{x}) = \\sum_{i=0}^{n} \\theta_{i} x_{i} = \\vec{\\theta}^{T} \\vec{x}$$\n",
      "Where we take $x_0 = 1$, and let $\\vec{\\theta} \\in \\mathbb{R}^{n+1}$ be the parameter vector.<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cost function $J : \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}$ has form:<br>\n",
      "$$J(\\vec{\\theta}) = \\frac{1}{2 m}\\sum_{i=1}^{m}(h_\\vec{\\theta}(\\vec{x}_{i}) - y_i)^2 = \\frac{1}{2 m}\\sum_{i=1}^{m}(\\vec{\\theta}^{T} \\vec{x} - y_i)^2$$<br>\n",
      "\n",
      "We want to minimize the cost function $J$ by varying the parameter vector $\\vec{\\theta}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, we can analytically solve this problem by solving the system of equations for $(\\theta_0, \\theta_1, \\dotsc, \\theta_{n})^T$ given by $\\frac{\\partial}{\\partial \\theta_i}{J(\\vec{\\theta})} = 0$ for all $i \\in \\{0,1,...,n\\}$.<br>\n",
      "\n",
      "\n",
      "We can alternatively use an optimization algorithm like gradient descent, and we can recycle it for other min/max problems; so I'll go over that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One method to find the parameter vector is the gradient descent algorithm.<br><br>\n",
      "\n",
      "The idea in gradient descent is similar to the idea in single variable calculus:<br>\n",
      "<img src=\"files/images/deriv.png\"><br>\n",
      "Where we know that $\\frac{d}{dx} f(x) = 0$ at the extrema (points A and B), and $\\frac{d}{dx} f(x)$ can otherwise be used direct us toward the nearest (local) minimum/maximum (e.g. the slope of the lines at points C, D, E, F).<br><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a multivariate function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$, we use the vector differential, or gradient: $\\vec{\\nabla} f(x)$ to guide us to a minimum.<br>For example, in:\n",
      "<img src=\"files/images/gradient.png\" width=\"400px\"><br>\n",
      "A gradient vector field is shown projected beneath a plot of the function $f(x,y) = \u2212(\\cos^2 x + \\cos^2 y)^2.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the vectors are pointed to the direction of maximum ascent--i.e. away from the minimum, thus if we want to minimize our cost function $J$ in the linear regression problem, we need to look toward the opposite direction: $-\\vec{\\nabla} f(x)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how we actually follow that direction, consider the Taylor expansion of $f(\\vec{x}_0 + h\\vec{\\epsilon})$ where $\\epsilon \\in \\mathbb{R}^{n+1}$ is a unit-vector that we want to minimize in the direction of if we start at vector $\\vec{x}_0$, and $h \\in \\mathbb{R}$ is a constant \"step-size\":<br>\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) = f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} + h^2\\text{(const. error)}\n",
      "$$\n",
      "<br><br>\n",
      "If we take $h$ to be very small such that $h^2$ is even smaller, then let's regard $h^2\\text{(const. error)} \\approx 0$ so that:\n",
      "$$\n",
      "f(\\vec{x}_0 + h\\vec{\\epsilon}) \\approx f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to choose a vector $\\vec{\\epsilon}$ that minimizes $f(\\vec{x}_0 + h\\vec{\\epsilon})$.<br>\n",
      "i.e. that minimizes\n",
      "$f(\\vec{x}_0) + h\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon}$.<br><br>\n",
      "\n",
      "Note that $\\vec{\\nabla} f(\\vec{x}_0) \\cdot \\vec{\\epsilon} = \\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert \\lvert \\vec{\\epsilon} \\rvert \\cos \\delta$ where $\\delta$ is the angle between $\\vec{\\nabla} f(\\vec{x}_0)$ and $\\vec{\\epsilon}$.<br>\n",
      "By choosing unit-vector $\\vec{\\epsilon} = \\frac{-\\vec{\\nabla} f(\\vec{x}_0)}{\\lvert \\vec{\\nabla} f(\\vec{x}_0) \\rvert}$, we have $\\cos(\\delta) = -1$, its least value. <br><br>\n",
      "\n",
      "So taking:\n",
      "$\\vec{x_1} = \\vec{x}_0 + h\\vec{\\epsilon} = \\vec{x}_0 - h\\vec{\\nabla} f(\\vec{x}_0)$ for some step-size $h$,<br>\n",
      "and inspecting whether $f(\\vec{x_{i+1}}) - f(\\vec{x_{i}})$ reaches convergence within some threshold, we have the basic gradient descent algorithm.<br><br>\n",
      "\n",
      "To implement it with our cost function, we need the $i^\\text{th}$ component of $\\vec{\\nabla}J(\\vec{\\theta})$.<br>\n",
      "This is given by:  $\\frac{\\partial}{\\partial \\theta_i} J(\\vec{\\theta}) = \\frac{1}{m} \\sum_{j=1}^{m}x_{j,i}(\\vec{\\theta}^T \\cdot \\vec{x}_j - y_j)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also see: http://mathworld.wolfram.com/MethodofSteepestDescent.html <br>\n",
      "\n",
      "Image sources:<br>\n",
      "http://www.themathpage.com/acalc/calc_IMG/060.png<br>\n",
      "http://en.wikipedia.org/wiki/File:Gradient99.png<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementation and Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mv_linear_regression(training_set)\n",
      "    # Training_set assumed to be a dataframe\n",
      "    # For which the last column is the output variable.\n",
      "    \n",
      "end\n",
      "\n",
      "function plot_lin_reg()\n",
      "    # For low dimensional data, plot data and the regression line.\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "# methods for generic function plot_lin_reg\n",
        "plot_lin_reg()"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using RDatasets\n",
      "\n",
      "iris = data(\"datasets\", \"iris\")\n",
      "println(\"Loaded iris dataset.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded iris dataset.\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's predict petal width as a function of sepal.length, sepal.width, and petal.length.\n",
      "# data_set = iris[:(Species .== \"setosa\"), :][[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]]\n",
      "\n",
      "# Let's predict petal width as a function of sepal.length, sepal.width.\n",
      "data_set = iris[:(Species .== \"setosa\"), :][[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Width\"]]\n",
      "\n",
      "head(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "6x3 DataFrame:\n",
        "        Sepal.Length Sepal.Width Petal.Width\n",
        "[1,]             5.1         3.5         0.2\n",
        "[2,]             4.9         3.0         0.2\n",
        "[3,]             4.7         3.2         0.2\n",
        "[4,]             4.6         3.1         0.2\n",
        "[5,]             5.0         3.6         0.2\n",
        "[6,]             5.4         3.9         0.4\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}